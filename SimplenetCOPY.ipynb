{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7280d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from __future__ import print_function\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427bcfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "wd = 1e-6\n",
    "ne = 60\n",
    "nsc = 20\n",
    "gamma = 0.32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#To get reproductible experiment\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1f4c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913725490196078, 0.4823529411764706, 0.4466666666666667), (0.24705882352941178, 0.24352941176470588, 0.2615686274509804)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4913725490196078, 0.4823529411764706, 0.4466666666666667), (0.24705882352941178, 0.24352941176470588, 0.2615686274509804)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# Création de l'ensemble de validation\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
    "split=5000\n",
    "num_train=50000\n",
    "indices = list(range(num_train))\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "trainset = torch.utils.data.Subset(trainset,train_idx)\n",
    "validset = torch.utils.data.Subset(validset,valid_idx)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2) #batch 128\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=100, shuffle=False, num_workers=2)\n",
    "#Création de l'ensemble de test\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93dd8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplenet(nn.Module):\n",
    "    def __init__(self, classes=10, simpnet_name='simplenet'):\n",
    "        super(simplenet, self).__init__()\n",
    "        #print(simpnet_name)\n",
    "        self.features = self._make_layers() #self._make_layers(cfg[simpnet_name])\n",
    "        self.classifier = nn.Linear(256, classes)\n",
    "        self.drp = nn.Dropout(0.1)\n",
    "\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "\n",
    "        # print(own_state.keys())\n",
    "        # for name, val in own_state:\n",
    "        # print(name)\n",
    "        for name, param in state_dict.items():\n",
    "            name = name.replace('module.', '')\n",
    "            if name not in own_state:\n",
    "                # print(name)\n",
    "                continue\n",
    "            if isinstance(param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            print(\"STATE_DICT: {}\".format(name))\n",
    "            try:\n",
    "                own_state[name].copy_(param)\n",
    "            except:\n",
    "                print('While copying the parameter named {}, whose dimensions in the model are'\n",
    "                      ' {} and whose dimensions in the checkpoint are {}, ... Using Initial Params'.format(\n",
    "                    name, own_state[name].size(), param.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "\n",
    "        #Global Max Pooling\n",
    "        out = F.max_pool2d(out, kernel_size=out.size()[2:]) \n",
    "        # out = F.dropout2d(out, 0.1, training=True)\n",
    "        out = self.drp(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self):\n",
    "\n",
    "        model = nn.Sequential(\n",
    "                             nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(64, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                             nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(512, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
    "                             nn.BatchNorm2d(2048, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "\n",
    "                             nn.Conv2d(2048, 256, kernel_size=[1, 1], stride=(1, 1), padding=(0, 0)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "                             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False),\n",
    "                             nn.Dropout2d(p=0.1),\n",
    "\n",
    "\n",
    "                             nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)),\n",
    "                             nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True),\n",
    "                             nn.ReLU(inplace=True),\n",
    "\n",
    "                            )\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb61d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simplenet()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c193473",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.1, rho=0.9, eps= 0.001, weight_decay=0.002) #params= [140655709219720, 140655709219792, 140655709219936, 140655709220080, 140655709220440, 140655709220512, 140655709220656, 140655709220800, 140655709219288, 140655709302856, 140655709303000, 140655709303144, 140655709303432, 140655709303504, 140655709303576, 140655709303648, 140655709303936, 140655709304008, 140655709304080, 140655709304152, 140655709304440, 140655709304512, 140655709304584, 140655709304656, 140655709304944, 140655709305016, 140655709305088, 140655709305160, 140655709305448, 140655709305520, 140655709305592, 140655709305664, 140655709305952, 140655709306024, 140655709306096, 140655709306168, 140655709306456, 140655709306528, 140655709306600, 140655709306672, 140655607672976, 140655607673048, 140655607673120, 140655607673192, 140655607673480, 140655607673552, 140655607673624, 140655607673696, 140655607673984, 140655607674056, 140655607674200, 140655607674344, 140655607674704, 140655607674776])\n",
    "lr_sc = lr_scheduler.StepLR(optimizer, step_size=nsc, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch,trainloader):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(enumerate(trainloader), total=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in loop:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        loop.set_description(f\"Epoch [{epoch}]\")\n",
    "        loop.set_postfix(acc=correct/total)\n",
    "\n",
    "def test(epoch,validloader):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(enumerate(validloader), total=len(validloader))\n",
    "        for batch_idx, (inputs, targets) in loop:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loop.set_postfix(acc=correct/total)\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "        \n",
    "def testfinal(testloader):\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.t7')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(enumerate(testloader), total=len(testloader))\n",
    "        for batch_idx, (inputs, targets) in loop:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loop.set_postfix(acc=correct/total)\n",
    "        print(\"\\nFinal accuracy on the test set : \",correct/total)\n",
    "\n",
    "best_acc = 0       \n",
    "\n",
    "for epoch in range(0, ne):\n",
    "    train(epoch,trainloader)\n",
    "    test(epoch,validloader)\n",
    "    lr_sc.step()\n",
    "\n",
    "testfinal(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c543f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
